{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 [-0.11379394 -0.05179826  0.13815019  0.38340995] 1.0 False {}\n",
      "200 [-0.0872247   0.78211613 -0.00569919 -1.22691841] 1.0 False {}\n",
      "300 [-0.08352774 -1.16030341  0.21613116  1.99162   ] 1.0 True {}\n",
      "400 [-0.20695815 -0.62769005  0.10958747  0.88750773] 1.0 False {}\n",
      "500 [-0.14216951 -0.5649626  -0.05346357  0.00428762] 1.0 False {}\n",
      "600 [-0.14212395 -0.407813    0.12835854  0.54606547] 1.0 False {}\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "observation = env.reset()\n",
    "\n",
    "for i in range(1000):\n",
    "    env.render()\n",
    "    action = env.action_space.sample() # your agent here (this takes random actions)\n",
    "    observation, reward, done, info = env.step(action)\n",
    "\n",
    "    if (i+1)%100 == 0:\n",
    "        print(i+1, observation, reward, done, info)\n",
    "        \n",
    "    if done:\n",
    "        observation = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(16, activation='relu', input_shape=(4,)))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(2, activation='linear')) # 0 or 1\n",
    "\n",
    "model.compile(loss='mse', optimizer=RMSprop(lr=0.001)) # lr=0.001 (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.array([1,0,1,0])\n",
    "\n",
    "model.predict(s.reshape(1,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "memory = collections.deque(maxlen = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(32):\n",
    "    action = env.action_space.sample() # np.random.randint(2)\n",
    "    s2, r, done, _ = env.step(action)\n",
    "    \n",
    "    memory.append([s, action, r, done, s2])\n",
    "    \n",
    "    s = env.reset() if done else s2 # done 이면 리셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(memory[0], memory[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1.0 # 처음에는 탐험만 수행한다. (랜덤 행동)\n",
    "gamma = 0.99 # 감쇠율 (discount factor, 미래 보상을 얼마나 중요시할 지를 결정)\n",
    "returns = [] # 에피소드 당 총보상값을 저장한다\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 총 10000번의 에피소드를 진행한다.\n",
    "for episode in range(300):\n",
    "    \n",
    "    total_reward = 0 # 에피소스당 총보상값\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for i in range(200): # 한 에피소드당 최대 200번만 행동한다\n",
    "        \n",
    "        ### 탐험 확률 지정 (1 부터 시작해서 점점 낮아지다 최소값은 1%)\n",
    "        epsilon = 0.01 + (1-0.01)*np.exp(-0.0001*count)\n",
    "        count += 1\n",
    "        \n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample() # 랜덤 행동\n",
    "        else:\n",
    "            action = np.argmax(model.predict(s.reshape(1,4))[0]) # Q값이 높은 행동 선택\n",
    "            \n",
    "        s2, r, done, _ = env.step(action)\n",
    "        \n",
    "        total_reward += r\n",
    "        \n",
    "        memory.append([s, action, r, done, s2])\n",
    "        \n",
    "        ### 학습 (배치크기는 32)\n",
    "        indices = np.random.choice(len(memory), 32, replace=False)\n",
    "        samples = [memory[i] for i in indices]\n",
    "        \n",
    "        X = np.zeros([32,4]) # 입력 상태값\n",
    "        y = np.zeros([32,2]) # 목표 Q값\n",
    "        \n",
    "        for i, sample in enumerate(samples): # sample -> [s,a,r,done,s2]\n",
    "            X[i] = sample[0]\n",
    "            y[i] = model.predict(sample[0].reshape(1,4))[0]\n",
    "            \n",
    "            if sample[3] == True: # done\n",
    "                y[i][sample[1]] = sample[2]\n",
    "            else:\n",
    "                y[i][sample[1]] = sample[2] + gamma*np.max(model.predict(sample[-1].reshape(1,4))[0])\n",
    "                \n",
    "        model.fit(X, y, epochs=1, verbose=False)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        else:\n",
    "            s = s2\n",
    "            \n",
    "    print('Episode: %d, Reward: %d, Epsilon: %.5f' % (episode+1, total_reward, epsilon))\n",
    "    returns.append(total_reward)\n",
    "    \n",
    "    if total_reward == 200:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(returns), len(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(3):\n",
    "    env.reset()\n",
    "    \n",
    "    done = False\n",
    "    n = 0\n",
    "    while not done:\n",
    "        #a = np.random.randint(2)\n",
    "        a = np.argmax(model.predict(s.reshape(1,4))[0])\n",
    "        s, r, done, info = env.step(a)\n",
    "        n += 1\n",
    "        env.render()\n",
    "        #print(s, r, done, info)\n",
    "        \n",
    "    print('Epoch: %d, Count: %d' % (epoch+1, n))\n",
    "    print(s, r, done, info)\n",
    "    #input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
